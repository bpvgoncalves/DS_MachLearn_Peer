# Machine Learning (Course Project)
**Data Science -> Practical Machine Learning -> Peer Assessment**  
<br><br>  

```{r Setup, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Setup
startTime <- Sys.time()
require(knitr)
require(tools)
require(caret)
require(randomForest)
require(kernlab)
require(MASS)
#this will allow parallel processing on model fitting on unix-based OS's.
if (.Platform$OS.type=="unix") require(doMC) 
opts_chunk$set(echo=FALSE, results="markup")
opts_knit$set(verbose=TRUE)
set.seed(112358) # the single digit numbers of Fibonacci sequence!
```

-------------------
### Executive Summary  

Executive summary goes here!  
<br>

-------------------
### The problem

The main purpose of this work is trying to answer the question about the possibility to use data collected by sensors included in user wearable devices to assess whether or not the user is preforming a given physical activity in a correct way or, on the other hand, if the user is preforming it wrongly (with potential negative health impacts, such as lesions).  
As original authors said (Velloso et al, 2013), this is not the traditional activity recognition work which tries to identify **which** activity the user is preforming, but a more complex analysis trying to identify **how** a given activity is being preformed.  
<br>

-------------------
### The Data: Loading, Preprocessing and Exploratory Analysis   

We will use the data provided by _Qualitative Activity recognition of Weight Lifting Exercices_ team (Velloso et al, 2013), which includes the data collected by sensors worn by 6 different users while preforming weight lifting exercises in a controlled environment, either preforming the activity correctly or making one of four common mistakes. Data is then labeled accordingly as **A** (no execution errors) or **B** to **E** (for each of the execution errors being tested).
As a first step we will start by downloading the training and submission cases from the given assignment URLs. Once we make sure we have the right files by comparing their MD5 hash with a pre-computed one, they are loaded into memory.

```{r Loading}
# Loading
kRemote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
kFile   <- "data/train.csv"
kMd5    <- "56926c78af383dcdc2060407942e52e9"
    
if (!file.exists(kFile)){
    # File is not present at the working directory. Lets download it!
    method <-"auto"  # Default method: shall be fine for MS Windows (untested!)
    if (.Platform$OS.type=="unix") method <- "curl"  # Use for unix-like systems
    download.file(kRemote, kFile, method, FALSE, "wb")
}

if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    # File is now present (it was before or it was sucessfully downloaded).
    # Lets check if it is the expected file and no curruption ocurred during 
    # download. I'll compare it's MD5 hash with a precomputed one.
    if (as.vector(md5sum(kFile))!=kMd5){
        stop("File is not correct!")
    } else {
        # The correct file shall be present. 
        trainData <- read.csv(kFile, stringsAsFactors=FALSE)
        #print("Training Data sucessfully loaded!")
    }
}

kRemote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
kFile   <- "data/test.csv"
kMd5    <- "bc4174f3ec5dfcc5c570a1d2709272d9"

if (!file.exists(kFile)){
    method <-"auto"
    if (.Platform$OS.type=="unix") method <- "curl"
    download.file(kRemote, kFile, method, FALSE, "wb")
}

if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    if (as.vector(md5sum(kFile))!=kMd5){
        stop("File is not correct!")
    } else {
        testData <- read.csv(kFile, stringsAsFactors=FALSE)
        #print("Testing Data sucessfully loaded!")
    }
}
rm(kRemote, kFile, kMd5)

cat("Training data:", dim(trainData), "\nTesting data:", dim(testData))

#Preprocessing
trainClean <- trainData[, -c(12:36,50:59,69:83,87:101,103:112,125:139,141:150)]
quizClean <- testData[, -c(12:36,50:59,69:83,87:101,103:112,125:139,141:150)]
trainClean$classe <- factor(trainClean$classe)

```

There are two files with `r dim(trainData)[2]` variables and `r dim(trainData)[1]` records for the training dataset and `r dim(testData)[1]` records for the test cases to be submitted. We'll keep the test cases apart until the prediction phase, later in this document. 
After some data cleanup to remove columns mostly with empty or NA records, we end up with only `r dim(trainClean)[2]` variables with the previous number of records. Each record belongs to one (and only one) `Classe` named from A to E, with the distribution below.
```{r ClasseSummary} 
summary(trainClean$classe)
rm(trainData, testData)
```

The next step is split the training dataset into 3 different sets: one for model training, another for model testing and a last one for model validation. About 40% of the samples will be used for training, and 30% for each of testing and validation sets.  
While this might look sub optimal and not complaint with recommended 60-40 distribution for training and testing, we will use the testing set to fit our third model, so we chose to have more cases there.

```{r DatasetSpliting}
trn_rows <- createDataPartition(trainClean$classe, p=0.40, list=FALSE)
trainSet <- trainClean[trn_rows, ]
tmpSet <- trainClean[-trn_rows, ]
tst_rows <- createDataPartition(tmpSet$classe, p=0.50, list=FALSE)
testSet <- tmpSet[tst_rows, ]
validSet <- tmpSet[-tst_rows, ]

rm(trainClean, tmpSet, trn_rows, tst_rows)

cat("Training set:", dim(trainSet), 
    "\nTesting set:", dim(testSet),
    "\nValidation set:", dim(validSet))

```

Looking into the variables' names present at the training set it is clear that some of them (X, user.name, raw.timestamp.part.1, ...) are not related to the data collected by the devices and only provide information about the record itself, such as exercise execution time or user name. Excluding those variables (first 7 columns) and the variable we want to predict (last column), leaves us with 52 potential predictor variables. 

We can also wonder if all those are really necessary. Using Principal Component Analysis to extract the main features from the dataset suggests that we can capture about 95% of data variability using 26 components (half the initial number of variables).

``` {r Feature Extraction}
#Feature Extraction
prePCA <- preProcess(trainSet[, 8:59], "pca")
prePCA
```

In spite of this potential noise reduction and model simplification, during the exploratory analysis we noticed an important decrease in models' accuracy when using PCA so we will not preform this transformation to the data before fitting the models and we will therefore keep the 52 variables.  
<br>

-------------------
### Model Selection

Given the nature of the problem being addressed: correctly classifying the record within one of five different classes, some models not suited for classification (eg, linear regresion) are automaticly excluded on our selection process.  
CONTINUE HERE...
<br>

-------------------
### Models Fitting



```{r ModelFitting, cache=TRUE}


if (.Platform$OS.type=="unix") registerDoMC(cores=4)

# **TODO**
# Fix RNG seeds for reproducibility on multicore processing
# **

fit1 <- train(classe~., 
              method     = "knn", 
              data       = trainSet[, 8:60], 
              preProcess = c("center", "scale"), 
              trControl  = trainControl(method="cv", number=16)
             )
fit2 <- train(classe~., 
              method     = "rf", 
              data       = trainSet[, 8:60], 
              preProcess = c("center", "scale"), 
              trControl  = trainControl(method="cv", number=16),
              ntree      = 128
             )
fit3 <- train(classe~., 
              method     = "qda", 
              data       = trainSet[, 8:60], 
              preProcess = c("center", "scale"), 
              trControl  = trainControl(method="cv", number=16)
             )

p1 <- predict(fit1, testSet)
p2 <- predict(fit2, testSet)
p3 <- predict(fit3, testSet)

cm1 <- confusionMatrix(p1, testSet$classe)
cm2 <- confusionMatrix(p2, testSet$classe)
cm3 <- confusionMatrix(p3, testSet$classe)
```


```{r ModelCombination, cache=TRUE}

trainSetComb <- data.frame(target=testSet$classe, 
                           f1=factor(p1), f2=factor(p2), f3=factor(p3))

fitComb <- train(target~f1+f2+f3,
                 method    = "lda", 
                 data      = trainSetComb, 
                 trControl = trainControl(method="cv", number=16)
                )

validSetEstim <- data.frame(f1=predict(fit1, validSet),
                            f2=predict(fit2, validSet),
                            f3=predict(fit3, validSet))
pComb <- predict(fitComb, validSetEstim)
cmComb <- confusionMatrix(pComb, validSet$classe)
```


```{r SubmissionSet}

write_files <- function(response){
    n = length(response)
    for(i in 1:n){
        filename <- paste0("./quizFiles/problem_id_",i,".txt")
        write.table(response[i], 
                    file      = filename, 
                    quote     = FALSE, 
                    row.names = FALSE, 
                    col.names = FALSE)
    }
}

quizSet <- data.frame(f1=predict(fit1, quizClean),
                      f2=predict(fit2, quizClean),
                      f3=predict(fit3, quizClean))
pQuiz <- predict(fitComb, quizSet)
write_files(pQuiz)

pQuiz
# [1] B A B A A E D B A A B C B A E E A B B B
# Levels: A B C D E
```

``` {r ModelCompare}

rsamples <- resamples(list(KNearNbour=fit1, RndForest=fit2, 
                           QuadDiscAn=fit3, Combined=fitComb))
summary(rsamples)
```

<br>

-------------------
### Reproducibility

This report has been made using R Markdown and the presented results should be fully reproducible. The following software environment has been used: 
- OS: Linux x86_64 (3.15.9-pclos1)
- R: 3.1.1
- RStudio: 0.98.481
- R packages: 
  - knitr (1.6)
  - tools (3.1.1)
  - caret (6.0-35):
      - lattice(0.20-29)
      - ggplot2 (1.0.0)
      - kernlab (0.9-19)
      - randomForest (4.6-10)
      - MASS (7.3-33)
  - doMC (1.3.3):
      - foreach (1.4.2)
      - iterators (1.0.7)
      - parallel (3.1.1)

```{r Final}
endTime <- Sys.time()
minutes <- difftime(endTime, startTime,units="mins")
seconds <- (minutes-trunc(minutes))*60
cat("Report generated on:", format(endTime, "%Y-%m-%d %H:%M:%S %Z"),
    "\nProcessing time:", trunc(minutes), "minutes and", round(seconds, 0), "seconds.")
```

All the code used to produce this report is available on [Github](https://github.com/bpvg/DS_MachLearn_Peer).  
<br>

-------------------
### References

Filzmoser, Peter _Linear and Nonlinear Methods for Regression and Classification and applications in R_. Vienna University of Technology, 2008  

Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome _The Elements of Statistical Learning_. Springer, 2008  

Shalizi, Cosma R. _Advanced Data Analysis from an Elementary Point of View_. Carnegie Mellon University, 2013  

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. _Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)_ . Stuttgart, Germany: ACM SIGCHI, 2013.  