# Machine Learning (Course Project)
**Data Science -> Practical Machine Learning -> Peer Assessment**  
<br><br>  

```{r Setup, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Setup
startTime <- Sys.time()
require(knitr)
require(tools)
require(caret)
require(randomForest)
require(kernlab)
require(MASS)
#this will allow parallel processing on model fitting on unix-based OS's.
if (.Platform$OS.type=="unix") require(doMC) 
opts_chunk$set(echo=FALSE, results="markup")
opts_knit$set(verbose=TRUE)
set.seed(112358) # the single digit numbers of Fibonacci sequence!
```

-------------------
### Executive Summary  

Executive summary goes here!  
<br>

-------------------
### The problem

The main purpose of this work is trying to answer the question about the possibility to use data collected by sensors included in user wearable devices to assess whether or not the user is preforming a given physical activity in a correct way or, on the other hand, if the user is preforming it wrongly (with potential negative health impacts, such as lesions).  
As original authors said (Velloso et al, 2013), this is not the traditional activity recognition work which tries to identify **which** activity the user is preforming, but a more complex analysis trying to identify **how** a given activity is being preformed.  
<br>

-------------------
### The Data: Loading, Preprocessing and Exploratory Analysis   

We will use the data provided by _Qualitative Activity recognition of Weight Lifting Exercices_ team (Velloso et al, 2013), which includes the data collected by sensors worn by 6 different users while preforming weight lifting exercises in a controlled environment, either preforming the activity correctly or making one of four common mistakes. Data is then labeled accordingly as **A** (no execution errors) or **B** to **E** (for each of the execution errors being tested).
As a first step we will start by downloading the training and submission cases from the given assignment URLs. Once we make sure we have the right files by comparing their MD5 hash with a pre-computed one, they are loaded into memory.

```{r Loading}
# Loading
kRemote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
kFile   <- "data/train.csv"
kMd5    <- "56926c78af383dcdc2060407942e52e9"
    
if (!file.exists(kFile)){
    # File is not present at the working directory. Lets download it!
    method <-"auto"  # Default method: shall be fine for MS Windows (untested!)
    if (.Platform$OS.type=="unix") method <- "curl"  # Use for unix-like systems
    download.file(kRemote, kFile, method, FALSE, "wb")
}

if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    # File is now present (it was before or it was sucessfully downloaded).
    # Lets check if it is the expected file and no curruption ocurred during 
    # download. I'll compare it's MD5 hash with a precomputed one.
    if (as.vector(md5sum(kFile))!=kMd5){
        stop("File is not correct!")
    } else {
        # The correct file shall be present. 
        trainData <- read.csv(kFile, stringsAsFactors=FALSE)
        #print("Training Data sucessfully loaded!")
    }
}

kRemote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
kFile   <- "data/test.csv"
kMd5    <- "bc4174f3ec5dfcc5c570a1d2709272d9"

if (!file.exists(kFile)){
    method <-"auto"
    if (.Platform$OS.type=="unix") method <- "curl"
    download.file(kRemote, kFile, method, FALSE, "wb")
}

if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    if (as.vector(md5sum(kFile))!=kMd5){
        stop("File is not correct!")
    } else {
        testData <- read.csv(kFile, stringsAsFactors=FALSE)
        #print("Testing Data sucessfully loaded!")
    }
}
rm(kRemote, kFile, kMd5)

cat("Training data:", dim(trainData), "\nTesting data:", dim(testData))

#Preprocessing
trainClean <- trainData[, -c(12:36,50:59,69:83,87:101,103:112,125:139,141:150)]
quizClean <- testData[, -c(12:36,50:59,69:83,87:101,103:112,125:139,141:150)]
trainClean$classe <- factor(trainClean$classe)

```

There are two files with `r dim(trainData)[2]` variables and `r dim(trainData)[1]` records for the training dataset and `r dim(testData)[1]` records for the test cases to be submitted. We'll keep the test cases apart until the prediction phase, later in this document. 
After some data cleanup to remove columns mostly with empty or NA records, we end up with only `r dim(trainClean)[2]` variables with the previous number of records. Each record belongs to one (and only one) `Classe` named from A to E, with the distribution below.
```{r ClasseSummary} 
summary(trainClean$classe)
rm(trainData, testData)
```

The next step is split the training dataset into 3 different sets: one for model training, another for model testing and a last one for model validation. About 40% of the samples will be used for training, and 30% for each of testing and validation sets.  
While this might look sub optimal and not complaint with recommended 60-40 distribution for training and testing, we will use the testing set to fit our third model, so we chose to have more cases there.

```{r DatasetSpliting}
trn_rows <- createDataPartition(trainClean$classe, p=0.40, list=FALSE)
trainSet <- trainClean[trn_rows, ]
tmpSet <- trainClean[-trn_rows, ]
tst_rows <- createDataPartition(tmpSet$classe, p=0.50, list=FALSE)
testSet <- tmpSet[tst_rows, ]
validSet <- tmpSet[-tst_rows, ]

rm(trainClean, tmpSet, trn_rows, tst_rows)

cat("Training set:", dim(trainSet), 
    "\nTesting set:", dim(testSet),
    "\nValidation set:", dim(validSet))

```

Looking into the variables' names present at the training set it is clear that some of them (X, user.name, raw.timestamp.part.1, ...) are not related to the data collected by the devices and only provide information about the record itself, such as exercise execution time or user name. Excluding those variables (first 7 columns) and the variable we want to predict (last column), leaves us with 52 potential predictor variables. 

We can also wonder if all those are really necessary. Using Principal Component Analysis to extract the main features from the dataset suggests that we can capture about 95% of data variability using 26 components (half the initial number of variables).

``` {r Feature Extraction}
#Feature Extraction
prePCA <- preProcess(trainSet[, 8:59], "pca")
prePCA
```

In spite of this potential noise reduction and model simplification, during the exploratory analysis we noticed an important decrease in models' accuracy when using PCA so we will not preform this transformation to the data before fitting the models and we will therefore keep the 52 variables.  
<br>

-------------------
### Model Selection

Given the nature of the problem being addressed: correctly classifying the record within one of five different classes, some models not suited for classification (eg, linear regression) were automatically excluded on our selection process.  
Most authors agree it doesn't exist a "best model" for all problems and situations, so we cannot be sure about the best _a priori_ model to use for this problem. We then choose to use three different models to see which one is the best for this task: the **K-Nearest Neighborhood (KNN)** used to classify each observation accordingly to the know classifications of the _k_ nearest observations in the features space, the powerful (yet black-box-styled) **Random Forest (RF)** getting its strength from the ensembling techniques applied to the individual decision trees, and, the **Quadratic Discriminant Analysis (QDA)**, a method similar to linear discriminant analysis which tries to find a combination of features able to identify each class, but where each class space is delimited by a quadratic function instead of a linear function.  
At the end, the outputs of the first two models will be combined in a fourth model, trying to get the best of each of the initial models. A **Linear Discriminant Analysis (LDA)** model will be used here to put these two together. QDA model will be excluded from this phase because it has lower Accuracy and will make the combined model less accurate than the RF model alone.  
For the remaining of the document, we will use Accuracy, the probability of getting the right outcome, as the measure of model quality.  
<br>

-------------------
### Models Fitting

It is now time to start the model fitting process.  

```{r ModelFitting, cache=TRUE}


if (.Platform$OS.type=="unix") registerDoMC(cores=4)

# Create a random number list to allow reproducibility while using multicore 
# processing. Since the computations are preformed in parallel and each run 
# execution time may be affected by external factors, it is not guaranteed the 
# RNG state will be the same for every script run.
# We use a vector of seeds to homogenize the results on every run.

set.seed(1321345589) # the double digit numbers of Fibonacci sequence!
s <- vector(mode = "list", length = 17)
for(i in 1:16) s[[i]] <- sample.int(9999, 5) # pick 5 numbers from 0 to 9999
s[[17]] <- sample.int(9999, 1)

fit1 <- train(classe~., 
              method     = "knn", 
              data       = trainSet[, 8:60], 
              preProcess = c("center", "scale"), 
              trControl  = trainControl(method="cv", number=16, seeds=s),
              tuneGrid   = data.frame(k=c(1, 3, 5, 7, 9))
             )
fit2 <- train(classe~., 
              method     = "rf", 
              data       = trainSet[, 8:60], 
              preProcess = c("center", "scale"), 
              trControl  = trainControl(method="cv", number=16, seeds=s),
              ntree      = 64, 
              tuneGrid   = data.frame(mtry=c(2, 4, 8, 16, 32))
             )
fit3 <- train(classe~., 
              method     = "qda", 
              data       = trainSet[, 8:60], 
              preProcess = c("center", "scale"), 
              trControl  = trainControl(method="cv", number=16)
             )

p1 <- predict(fit1, testSet)
p2 <- predict(fit2, testSet)
p3 <- predict(fit3, testSet)

cm1 <- confusionMatrix(p1, testSet$classe)
cm2 <- confusionMatrix(p2, testSet$classe)
cm3 <- confusionMatrix(p3, testSet$classe)
```


```{r ModelCombination, cache=TRUE}

trainSetComb <- data.frame(target=testSet$classe, 
                           f1=factor(p1), f2=factor(p2))

fitComb <- train(target~f1+f2,
                 method    = "lda", 
                 data      = trainSetComb, 
                 trControl = trainControl(method="cv", number=16)
                )

validSetEstim <- data.frame(f1=predict(fit1, validSet),
                            f2=predict(fit2, validSet))
pComb <- predict(fitComb, validSetEstim)
cmComb <- confusionMatrix(pComb, validSet$classe)
```

#### Parameters
Models were mostly fitted using their default parameters, but some changes were made to try to getting better accuracy or less processing time.
Starting with KNN, we tuned the model to check more k neighbors than it will do by default to try to get more Accuracy. In this case, k=1 and k=3 were tested together with the default 5, 7 and 9. According to the results obtained, k=1 provides better Accuracy.  
As long as the RF is concerned, we reduced the number of trees from the default 500 to 64. This grants faster processing but does not penalize too much the Accuracy. According to our findings during the exploratory phase, marginal Accuracy gains quickly decrease while more trees are added. We found 64 as a good compromise between models Accuracy and processing speed. We also changed the number of variables samples evaluated at each node, checking for 2, 4, 8, 16 and 32 variables instead of the original 2, 27 and 52.  
Traversal to all models, we can find the pre-processing settings. In all cases variables were centered and scaled.  

#### Cross-Validation
All models were subject to 2 different levels of cross validation.  
The first level was computed during the fitting process itself. To accomplish this, 16 different random sub-samples were created from the training dataset, using about 75% of the actual number of records for training and 25% for validation. Then the model were fitted on each of these sub-samples and the Accuracy computed. The summary of the results are on the table below.  

``` {r ModelCompare}
rsamples <- resamples(list(KNN=fit1, RF=fit2, QDA=fit3, Comb=fitComb))
summary(rsamples)$statistics$Accuracy
```

The second level of cross-validation is preformed using the fitted models to predict the known class for the 'Testing Set' (for KNN, RF and QDA models) records and for the 'Validation Set' (for the Combined model) and compare the prediction  with the actual class. The results, also measured by Accuracy, are the following: 

``` {r TestingSetError, fig.height=5, fig.width=12}
CV2 <- c(as.numeric(cm1$overall[1]), as.numeric(cm2$overall[1]), 
         as.numeric(cm3$overall[1]), as.numeric(cmComb$overall[1]))
names(CV2) <- c("KNN", "RF", "QDA", "Comb")
print(list(Accuracy=CV2))

#Chart
par(mar=c(2, 4, 2, 1), bty="n", mfrow=c(2, 2))
hist(rsamples$values[[2]], breaks=seq(0.85, 1, len=16), col="red", main="KNN", xlab="Accuracy")
hist(rsamples$values[[4]], breaks=seq(0.85, 1, len=16), col="blue", main="RF", xlab="Accuracy")
hist(rsamples$values[[6]], breaks=seq(0.85, 1, len=16), col="gold2", main="QDA", xlab="Accuracy")
hist(rsamples$values[[8]], breaks=seq(0.85, 1, len=16), col="forestgreen", main="Comb (KNN+RF)", xlab="Accuracy")
```

**Fig.1** - Distribution of each model Accuracy computed during the fitting process.  

We can see the Accuracy obtained on the 'Testing Set' and 'Validation Set' is very similar to the mean and median expected Accuracy computed during model fitting. It seems models are not over-fitted to the training data and can 'safely' be used to predict classes on new datasets.  
<br>

-------------------
### Models' Results

Once we finished with the previous steps and we have our final model we can then preform the last final step: use this final model to predict the 20 submission cases.  
The results are the following:

```{r SubmissionSet}

write_files <- function(response){
    n = length(response)
    for(i in 1:n){
        filename <- paste0("./quizFiles/problem_id_",i,".txt")
        write.table(response[i], 
                    file      = filename, 
                    quote     = FALSE, 
                    row.names = FALSE, 
                    col.names = FALSE)
    }
}

quizSet <- data.frame(f1=predict(fit1, quizClean),
                      f2=predict(fit2, quizClean))
pQuiz <- predict(fitComb, quizSet)
write_files(pQuiz)

pQuiz
# Quiz Submisison:
# [1] B A B A A E D B A A B C B A E E A B B B
# Levels: A B C D E
```

After submission, it resulted in 20 right predictions out of 20 records to predict. 
Playing around with the Poisson distribution, an assuming an average 'miss ratio' of 1.5 predictions out of every 100 attempts, we can compute a `r ppois(0, 20*1.5/100, lower=TRUE) ` probability of the 20/20 performance achieved (ie, 0 misses in 20 attempts, given a probability of missing 1.5 classifications in every 100).  
<br>

-------------------
### Reproducibility

This report has been made using R Markdown and the presented results should be fully reproducible. The following software environment has been used: 
- OS: Linux x86_64 (3.15.9-pclos1)
- R: 3.1.1
- RStudio: 0.98.481
- R packages: 
  - knitr (1.6)
  - tools (3.1.1)
  - caret (6.0-35):
      - lattice(0.20-29)
      - ggplot2 (1.0.0)
      - kernlab (0.9-19)
      - randomForest (4.6-10)
      - MASS (7.3-33)
  - doMC (1.3.3):
      - foreach (1.4.2)
      - iterators (1.0.7)
      - parallel (3.1.1)

```{r Final}
endTime <- Sys.time()
minutes <- difftime(endTime, startTime,units="mins")
seconds <- (minutes-trunc(minutes))*60
cat("Report generated on:", format(endTime, "%Y-%m-%d %H:%M:%S %Z"),
    "\nProcessing time:", trunc(minutes), "minutes and", round(seconds, 0), "seconds.")
```

All the code used to produce this report is available on [Github](https://github.com/bpvg/DS_MachLearn_Peer).  
<br>

-------------------
### References

Filzmoser, Peter _Linear and Nonlinear Methods for Regression and Classification and applications in R_. Vienna University of Technology, 2008  

Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome _The Elements of Statistical Learning_. Springer, 2008  

Shalizi, Cosma R. _Advanced Data Analysis from an Elementary Point of View_. Carnegie Mellon University, 2013  

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. _Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)_ . Stuttgart, Germany: ACM SIGCHI, 2013.  