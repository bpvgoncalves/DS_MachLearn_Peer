# Machine Learning (Course Project)
**Data Science -> Practical Machine Learning -> Peer Assessment**  
<br><br>  

```{r Setup, echo=FALSE, results='hide'}
# Setup
require(knitr)
require(tools)
require(caret)
require(randomForest)
require(ipred)
require(plyr)
#this will allow parallel processing on model fitting on unix-based OS's.
if (.Platform$OS.type=="unix") require(doMC) 
opts_chunk$set(echo=FALSE, results="markup")
opts_knit$set(verbose=TRUE)
set.seed(112358) # the single digit numbers of Fibonacci sequence!
```

-------------------
### Executive Summary  


-------------------
### Data Loading, Preprocessing and Exploratory Analysis   

We'll start by downloading the training and testing cases from the given assignment URLs. Once we make sure we have the right files, they are loaded into memory.

```{r Loading}
# Loading
kRemote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
kFile   <- "train.csv"
kMd5    <- "56926c78af383dcdc2060407942e52e9"
    
if (!file.exists(kFile)){
    # File is not present at the working directory. Lets download it!
    method <-"auto"  # Default method: shall be fine for MS Windows (untested!)
    if (.Platform$OS.type=="unix") method <- "curl"  # Use for unix-like systems
    download.file(kRemote, kFile, method, FALSE, "wb")
}

if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    # File is now present (it was before or it was sucessfully downloaded).
    # Lets check if it is the expected file and no curruption ocurred during 
    # download. I'll compare it's MD5 hash with a precomputed one.
    if (as.vector(md5sum(kFile))!=kMd5){
        stop("File is not correct!")
    } else {
        # The correct file shall be present. 
        trainData <- read.csv(kFile, stringsAsFactors=FALSE)
        #print("Training Data sucessfully loaded!")
    }
}

kRemote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
kFile   <- "test.csv"
kMd5    <- "bc4174f3ec5dfcc5c570a1d2709272d9"

if (!file.exists(kFile)){
    method <-"auto"
    if (.Platform$OS.type=="unix") method <- "curl"
    download.file(kRemote, kFile, method, FALSE, "wb")
}

if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    if (as.vector(md5sum(kFile))!=kMd5){
        stop("File is not correct!")
    } else {
        testData <- read.csv(kFile, stringsAsFactors=FALSE)
        #print("Testing Data sucessfully loaded!")
    }
}
rm(kRemote, kFile, kMd5)

cat("Training dataset:", dim(trainData), "\nTesting dataset:", dim(testData))

#Preprocessing
trainClean <- trainData[, -c(12:36,50:59,69:83,87:101,103:112,125:139,141:150)]
quizClean <- testData[, -c(12:36,50:59,69:83,87:101,103:112,125:139,141:150)]
trainClean$classe <- factor(trainClean$classe)

```

There are two files with `r dim(trainData)[2]` variables and `r dim(trainData)[1]` records for the training dataset and `r dim(testData)[1]` records for the test cases to be submitted. We'll keep the test cases apart until the prediction phase, later in this document. 
After some data cleanup to remove columns mostly with empty or NA records, we end up with only `r dim(trainClean)[2]` variables with the previous number of records. Each record belongs to one (and only one) `Classe` named from A to E, with the following distribution:
```{r ClasseSummary} 
summary(trainClean$classe)
rm(trainData, testData)
```

The next step is split the training dataset into 3 different sets: one for model training, another for model testing and a last one for model validation. About 40% of the samples will be used for training, and 30% for each of testing and validation sets.  
While this might look sub optimal and not complaint with recommended 60-40 distribution for training and testing, we will use the testing set to fit our third model and we chose to have more cases there.

```{r DatasetSpliting}
trn_rows <- createDataPartition(trainClean$classe, p=0.40, list=FALSE)
trainSet <- trainClean[trn_rows, ]
tmpSet <- trainClean[-trn_rows, ]
tst_rows <- createDataPartition(tmpSet$classe, p=0.50, list=FALSE)
testSet <- tmpSet[tst_rows, ]
validSet <- tmpSet[-tst_rows, ]

rm(trainClean, tmpSet, trn_rows, tst_rows)

cat("Training set:", dim(trainSet), 
    "\nTesting set:", dim(testSet),
    "\nValidation set:", dim(validSet))

```


-------------------
### Model Selection


-------------------
### Models Fitting



```{r ModelFitting}


if (.Platform$OS.type=="unix") registerDoMC(cores=2)
fit1 <- train(classe~., 
              method="knn", 
              preProcess=c("center", "scale"), 
              trControl = trainControl(method="cv"),
              data=trainSet[, 8:60]
             )
fit2 <- train(classe~., 
              method="rf", 
              data=trainSet[, 8:60], 
              preProcess=c("center", "scale"), 
              trControl = trainControl(method="cv"),
              ntree=250
             )

p1 <- predict(fit1, testSet)
p2 <- predict(fit2, testSet)

confusionMatrix(p1, testSet$classe)
confusionMatrix(p2, testSet$classe)
```


```{r ModelCombination}

trainSetComb <- data.frame(target=testSet$classe, f1=p1, f2=p2)

fitComb <- train(target~f1+f2,
                 method="treebag", 
                 trControl = trainControl(method="cv"),
                 data=trainSetComb 
                )

validSetEstim <- data.frame(f1=predict(fit1, validSet),
                            f2=predict(fit2, validSet))
pComb <- predict(fitComb, validSetEstim)
confusionMatrix(pComb, validSet$classe)

```


```{r SubmissionSet}

write_files <- function(response)
    n = length(response)
    for(i in 1:n){
        filename <- paste0("./quizFiles/problem_id_",i,".txt")
        write.table(response[i], 
                    file=filename, 
                    quote=FALSE, 
                    row.names=FALSE, 
                    col.names=FALSE)
    }
}

quizSet <- data.frame(f1=predict(fit1, quizClean),
                      f2=predict(fit2, quizClean))
pQuiz <- predict(fitComb, quizSet)
write_files(pQuiz)
pQuiz
summary(pQuiz)
```

-------------------
### References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. _Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13)_ . Stuttgart, Germany: ACM SIGCHI, 2013.